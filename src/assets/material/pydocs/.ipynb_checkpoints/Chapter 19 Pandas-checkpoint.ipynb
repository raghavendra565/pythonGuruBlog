{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Python Programming\n",
    "##### by Narendra Allam\n",
    "Copyright 2019\n",
    "\n",
    "# Chapter 19\n",
    "\n",
    "## Pandas\n",
    "\n",
    "#### Topics Covering\n",
    "* Pandas\n",
    "    * Series<br>\n",
    "        * Constructing from dictionaries<br>\n",
    "        * Custom Index<br>\n",
    "        * Data filtering<br>\n",
    "    * Data Frames<br>\n",
    "        * Constructing from a dictionary with values as lists<br>\n",
    "        * Custom indexing<br>\n",
    "        * Rearranging the columns<br>\n",
    "        * Setting values<br>\n",
    "        * Sum<br>\n",
    "        * Cummulative sum<br>\n",
    "        * Assigning a column to the dataframe<br>\n",
    "        * Adding a new column<br>\n",
    "        * Deleting a column<br>\n",
    "        * Slicing<br>\n",
    "        * Indexing and Advanced Indexing<br>\n",
    "        * Sorting<br>\n",
    "        * Transposing<br>\n",
    "        * Sort by<br>\n",
    "        * Concatenate<br>\n",
    "        * Merge<br>\n",
    "        * Join<br>\n",
    "        * Group By<br>\n",
    "        * Data Munging<br>\n",
    "            * Working Missing data<br>\n",
    "        * Reading Data from CSV, Excel, JSON<br>\n",
    "        * Writing Data to CSV, Excel, JSON<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command to install pandas\n",
    "\n",
    "pip install pandas (or) sudo apt-get install python-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to pandas Data Structures\n",
    "\n",
    "The heart of pandas is just the two primary data structures on which all transactions, which are generally made during the analysis of data, are centralized:\n",
    "- Series\n",
    "- DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "A Series is a one-dimensional array-like object containing an array of data, which can be any NumPy data type, and an associated array of data labels, functioning as its index.\n",
    "\n",
    "The structure of the Series object will be \n",
    "\n",
    "<img src='pandasSeries.jpg' width='150'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.Series([36, 32, 45, 30, 25, 40, 42], dtype=float)\n",
    "print (S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom indices\n",
    "S = pd.Series([36, 32, 45, 30, 25, 40, 42], index=['Sunday', 'Monday',\n",
    "                                                   'Tuesday', 'Wednessday',\n",
    "                                                   'Thursday', 'Friday',\n",
    "                                                   'Saturday'])\n",
    "print (S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S['Friday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom indices\n",
    "S = pd.Series([36, 32, 45, 30, 25, 40, 42], index=range(1, 8))\n",
    "print (S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data availability for indices\n",
    "expected_dates = [1, 3, 4, 6, 8, 9, 10]\n",
    "s1 = pd.Series(S, index=expected_dates)\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:-</b> NaN means Not a Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('MAX=', s1.max())\n",
    "print ('MIN=', s1.min())\n",
    "print ('AVG=', s1.mean())\n",
    "print ('STD=', s1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe( ) function  allows us to obtain a summary statistics at once.\n",
    "s1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>isnull( ) and notnull( ) </b>functions are very useful to identify the indexes without a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:- np.isnan()__ should be used when comparing with individual values in a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:-</b> NaN means Not a Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering 0s inplace of NaN\n",
    "s1[s1.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>isin( ) </b>is a function that evaluates the membership, that is, given a list of values, this function lets you know if these values are contained within the data structure. Boolean values that are returned can be very useful during the filtering of data within a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.isin([30,45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know all the values contained within the Series excluding duplicates, you can use the <b> unique( ) function</b>. The return value is an array containing the unique values in the Series, though not necessarily in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function similar to unique( ) is the <b> value_counts( ) function</b>, which not only returns the unique values but calculates occurrences within a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Index\n",
    "fruits = ['apples', 'oranges', 'cherries', 'pears', 'Mango']\n",
    "quantities = [20, 33, 52, 10, 40]\n",
    "S = pd.Series(quantities, index=fruits)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S['Mango']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar arithmatic\n",
    "print((S + 3) * 4)\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering fruits with quantity more than 30\n",
    "S[S > 30] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the quantities greater than 30 with custom list of values\n",
    "S[S > 30] = [99, 88, 77]\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "cities = {\"London\":   8615246,\n",
    "          \"Berlin\":   3562166, \n",
    "          \"Madrid\":   3165235, \n",
    "          \"Rome\":     2874038, \n",
    "          \"Paris\":    2273305, \n",
    "          \"Vienna\":   1805681, \n",
    "          \"Bucharest\":1803425, \n",
    "          \"Hamburg\":  1760433,\n",
    "          \"Budapest\": 1754000,\n",
    "          \"Warsaw\":   1740119,\n",
    "          \"Barcelona\":1602386,\n",
    "          \"Munich\":   1493900,\n",
    "          \"Milan\":    1350680}\n",
    "\n",
    "city_series = pd.Series(cities, dtype='uint32')\n",
    "print(city_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__All the cities with population greater than the average ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(city_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_series[city_series > np.average(city_series)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> All the cities with population < 1700000 and > 1300000? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_series[(city_series < 1700000) & (city_series > 1300000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population data available?\n",
    "my_cities = [\"London\", \"Paris\", \"Zurich\", \"Berlin\", \n",
    "             \"Stuttgart\", \"Hamburg\"]\n",
    "\n",
    "my_city_series = pd.Series(city_series, index=my_cities)\n",
    "print (my_city_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_city_series.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_city_series[my_city_series.isnull()] = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_city_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_city_series.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_series = pd.Series(my_city_series, dtype='uint64')\n",
    "print(city_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_series.index.str.startswith('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_series[city_series.index.str.startswith('s')] = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dtype of exisiting series\n",
    "s = pd.Series(my_city_series, dtype='uint32')\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.nan is not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([1, np.nan, 2])\n",
    "s2 = pd.Series([1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and Covariance                          \n",
    "Two important statistical calculations are correlation and covariance, expressed in pandas by the corr() and cov() functions. These kind of calculations normally involve two series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = pd.Series([1,2,3,4,4,3,2,1],['2006','2007','2008','2009','2010','2011','2012','2013'])\n",
    "seq.corr(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.cov(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = pd.Series([3,4,3,4,5,4,3,2],['2006','2007','2008','2009','2010','2011','2012','2013'])\n",
    "\n",
    "seq.corr(seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.cov(seq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Indexing and Leveling                                        \n",
    "Hierarchical indexing is a very important feature of pandas, as it allows you to have multiple levels of indexes on a single axis. It gives you a way to work with data in multiple dimensions while continuing to work in a two-dimensional structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mser = pd.Series(np.random.rand(8),index=[['white','white',\n",
    "                                           'white','blue',\n",
    "                                           'blue','red',\n",
    "                                           'red','red'], \n",
    "                                          ['up','down',\n",
    "                                           'right','up',\n",
    "                                           'down','up',\n",
    "                                           'down','left']])\n",
    "\n",
    "mser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mser.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can select the values for a given value of the first index, and you do it in the classic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mser['white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mser[:,'up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mser['white','up']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexing plays a critical role in reshaping data and group-based operations such as a pivot-table. For example, the data could be rearranged and used in a dataframe with a special function called <b>unstack()</b>. This function converts the series with a hierarchical index to a simple dataframe, where the second set of indexes is converted into a new set of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mser.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time-indexed data. \n",
    "\n",
    "- <b>Time stamps</b> reference particular moments in time (e.g., July 4th, 2015 at 7:00am).\n",
    "- <b>Time intervals and periods</b> reference a length of time between a particular beginning and end point; for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods comprising days).\n",
    "- <b>Time deltas or durations</b> reference an exact length of time (e.g., a duration of 22.56 seconds).\n",
    "\n",
    "#### Native Python dates and times: datetime and dateutil\n",
    "Python's basic objects for working with dates and times reside in the built-in datetime module. Along with the third-party dateutil module, you can use it to quickly perform a host of useful functionalities on dates and times. \n",
    "\n",
    "For example, you can manually build a date using the datetime type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime(year=2015, month=7, day=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using the dateutil module, you can parse dates from a variety of string formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "date = parser.parse(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a datetime object, you can do things like printing the day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of datetime and dateutil lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in.\n",
    "\n",
    "#### Typed arrays of times: NumPy's datetime64\n",
    "The weaknesses of Python's datetime format inspired the NumPy team to add a set of native\n",
    "time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit integers, and\n",
    "thus allows arrays of dates to be represented very compactly. The datetime64 requires a very\n",
    "specific input format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "date = np.array('2015-07-04', dtype=np.datetime64)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04 12:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04 12:59:59.50', 'ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table, drawn from the NumPy datetime64 documentation lists the available format codes along with the relative and absolute timespans that they can encode:\n",
    "\n",
    "<img src='pandas1.png' width='500'/>\n",
    "\n",
    "#### Dates and times in pandas: \n",
    "\n",
    "Pandas builds upon all the tools just discussed to provide a Timestamp object, which combines the ease-of-use of datetime and dateutil with the efficient storage and vectorized interface of numpy.datetime64. From a group of these Timestamp objects, Pandas can construct a DatetimeIndex that can be used to index data in a Series or DataFrame ; we'll see many examples of this below.\n",
    "\n",
    "For example, we can use Pandas tools to repeat the demonstration from above. We can parse a\n",
    "flexibly formatted string date, and use format codes to output the day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "date = pd.to_datetime(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + pd.to_timedelta(np.arange(12), 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Time Series: Indexing by Time\n",
    "Where the Pandas time series tools really become useful is when you begin to index data by\n",
    "timestamps. For example, we can construct a\n",
    "Series\n",
    "object that has time indexed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "'2015-07-04', '2015-08-04'])\n",
    "data = pd.Series([0, 1, 2, 3], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2014-07-04':'2015-07-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2015']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Time Series Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For time stamps, Pandas provides the Timestamp type. As mentioned before, it is essentially a replacement for Python's native datetime , but is based on the more efficient numpy.datetime64 data type. The associated Index structure is DatetimeIndex .\n",
    "- For time Periods, Pandas provides the Period type. This encodes a fixed-frequency interval based on numpy.datetime64 . The associated index structure is PeriodIndex .\n",
    "- For time deltas or durations, Pandas provides the Timedelta type. Timedelta is a more efficient replacement for Python's native datetime.timedelta type, and is based on numpy.timedelta64. The associated index structure is TimedeltaIndex ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most fundamental of these date/time objects are the Timestamp and DatetimeIndex\n",
    "objects. While these class objects can be invoked directly, it is more common to use the\n",
    "pd.to_datetime() function, which can parse a wide variety of formats. Passing a single date to pd.to_datetime() yields a Timestamp ; passing a series of dates by default yields a\n",
    "DatetimeIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\n",
    "'2015-Jul-6', '07-07-2015', '20150708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any DatetimeIndex can be converted to a PeriodIndex with the to_period() function with\n",
    "the addition of a frequency code; here we'll use 'D' to indicate daily frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.to_period('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TimedeltaIndex is created, for example, when a date is subtracted from another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular sequences: pd.date_range()\n",
    "\n",
    "To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for periods, and pd.timedelta_range() for time deltas. We've seen that Python's range() and NumPy's np.arange() turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly,\n",
    "pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the frequency is one day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', '2015-07-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the date range can be specified not with a start and endpoint, but with a startpoint and a number of periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', periods=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacing can be modified by altering the freq argument, which defaults to D . For example, here we will construct a range of hourly timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', periods=8, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create regular sequences of Period or Timedelta values, the very similar\n",
    "pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly\n",
    "periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.period_range('2015-07', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a sequence of durations increasing by an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequencies and Off sets\n",
    "Fundamental to these Pandas time series tools is the concept of a frequency or date off set. Just as we saw the D (day) and H (hour) codes above, we can use such codes to specify any desired frequency spacing. The following table summarizes the main codes available:\n",
    "\n",
    "<img src='pandas2.png' width='300' >\n",
    "\n",
    "The monthly, quarterly, and annual frequencies are all marked at the end of the specified period.\n",
    "\n",
    "By adding an S suffix to any of these, they instead will be marked at the beginning:\n",
    "```python\n",
    "\n",
    "        Code    Description     Code       Description\n",
    "        ------------------------------------------------------\n",
    "        MS      Month start      BMS      Business month start\n",
    "        QS     Quarter start     BQS      Business quarter start\n",
    "        AS      Year start       BAS      Business year start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix:\n",
    "\n",
    "- Q-JAN , BQ-FEB , QS-MAR , BQS-APR , etc.\n",
    "- A-JAN , BA-FEB , AS-MAR , BAS-APR , etc.\n",
    "\n",
    "In the same way, the split-point of the weekly frequency can be modified by adding a three-letter weekday code:\n",
    "\n",
    "- W-SUN , W-MON , W-TUE , W-WED , etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these short codes refer to specific instances of Pandas time series o sets, which can be found in the pd.tseries.offsets module. For example, we can create a business day o set directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import BDay\n",
    "pd.date_range('2015-07-01', periods=5, freq=BDay())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "The underlying idea of a DataFrame is based on spreadsheets. We can see the data structure of a DataFrame as tabular and spreadsheet-like. It contains an ordered collection of columns. Each column consists of a unique data type, but different columns can have different types, e.g. the first column may consist of integers, while the second one consists of boolean values and so on.\n",
    "\n",
    "A DataFrame has a row and column index; it's like a dict of Series with a common index. The structure of DataFrame will be as shown below.\n",
    "\n",
    "<img src='pandasDataFrame.jpg' width='300'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cities = {\"city_name\": [\"London\", \"Berlin\", \"Madrid\", \"Rome\", \n",
    "                   \"Paris\", \"Vienna\", \"Bucharest\", \"Hamburg\", \n",
    "                   \"Budapest\", \"Warsaw\", \"Barcelona\", \n",
    "                   \"Munich\", \"Milan\"],\n",
    "          \"population\": [8615246, 3562166, 3165235, 2874038,\n",
    "                         2273305, 1805681, 1803425, 1760433,\n",
    "                         1754000, 1805681, 1602386, 1805681,\n",
    "                         1350680],\n",
    "          \"country\": [\"England\", \"Germany\", \"Spain\", \"Italy\",\n",
    "                      \"France\", \"Austria\", \"Romania\", \n",
    "                      \"Germany\", \"Hungary\", \"Poland\", \"Spain\",\n",
    "                      \"Germany\", \"Italy\"]}\n",
    "\n",
    "city_frame = pd.DataFrame(cities)\n",
    "\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Indexing\n",
    "ordinals = [\"first\", \"second\", \"third\", \"fourth\",\n",
    "            \"fifth\", \"sixth\", \"seventh\", \"eigth\",\n",
    "            \"ninth\", \"tenth\", \"eleventh\", \"twelvth\",\n",
    "            \"thirteenth\"]\n",
    "\n",
    "city_frame = pd.DataFrame(cities, index=ordinals)\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging the columns\n",
    "city_frame = pd.DataFrame(city_frame,\n",
    "                          columns=[ \"country\",\n",
    "                                    \"city_name\", \n",
    "                                    \"population\"])\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame = city_frame.rename(columns = {'city_name':'cityname'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.rename(index = {'eigth':'EIGTH'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.groupby(city_frame.index).get_group('seventh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.rename(index = {'EIGTH':'eigth'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a column\n",
    "city_frame['cityname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a cell\n",
    "city_frame['cityname']['eigth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate syntax\n",
    "city_frame.cityname['eigth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Setting Values at() & iat()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.at['fourth', 'cityname'] = 'ROME_MODIFIED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.iat[2, 1] = 'MADRID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing and views\n",
    "##### loc(), iloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc['third': 'tenth', 'country':'cityname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc['third': 'tenth', 'cityname':'country':-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc['eigth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc['third': 'tenth' : 2, 'cityname':'country':-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Specific columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc[['first', 'sixth', 'tenth'], ['country', 'population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc['first':'fifth', ['country', 'population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.loc['fifth':'first':-1, ['country', 'population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.iloc[4::-1, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.iloc[2:9, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame['population'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum\n",
    "x = city_frame[\"population\"].cumsum()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Adding a new column\n",
    "city_frame['area'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = [1572, 891.85, 605.77, 1285, \n",
    "        105.4, 414.6, 228, 755, \n",
    "        525.2, 517, 101.9, 310.4, \n",
    "        181.8]\n",
    "\n",
    "city_frame[\"area\"] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['India', 'Hyderabad', 15000000, 700]], \n",
    "                  columns = ['country', 'cityname', 'population', 'area'],\n",
    "                  index = ['fourteenth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame = city_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame['extra'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.pop('extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.drop('area', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.drop('fourteenth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.drop(['fourteenth', 'thirteenth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Permenently removing a row:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.drop('fourteenth', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cities = {\"cityname\": [\"London\", \"Berlin\", \"Madrid\", \"Rome\", \n",
    "                   \"Paris\", \"Vienna\", \"Bucharest\", \"Hamburg\", \n",
    "                   \"Budapest\", \"Warsaw\", \"Barcelona\", \n",
    "                   \"Munich\", \"Milan\"],\n",
    "          \"population\": [8615246, 3562166, 3165235, 2874038,\n",
    "                         2273305, 1805681, 1803425, 1760433,\n",
    "                         1754000, 1805681, 1602386, 1805681,\n",
    "                         1350680],\n",
    "          \"country\": [\"England\", \"Germany\", \"Spain\", \"Italy\",\n",
    "                      \"France\", \"Austria\", \"Romania\", \n",
    "                      \"Germany\", \"Hungary\", \"Poland\", \"Spain\",\n",
    "                      \"Germany\", \"Italy\"],\n",
    "          \"area\"    : [1572, 891.85, 605.77, 1285, 105.4, 414.6, \n",
    "                       228, 755, 525.2, 517, 101.9, 310.4, 181.8]\n",
    "         }\n",
    "\n",
    "ordinals = [\"first\", \"second\", \"third\", \"fourth\",\n",
    "            \"fifth\", \"sixth\", \"seventh\", \"eigth\",\n",
    "            \"ninth\", \"tenth\", \"eleventh\", \"twelvth\",\n",
    "            \"thirteenth\"]\n",
    "\n",
    "city_frame = pd.DataFrame(cities, index=ordinals)\n",
    "\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sorting DataFrame on column 'population':__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "city_frame = city_frame.sort_values(\"population\", ascending=False)\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sorting DataFrame on multiple columns:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "city_frame = city_frame.sort_values([\"population\", 'area'], ascending=False)\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame on multiple columns but different sorting orders\n",
    "city_frame = city_frame.sort_values(['population', 'area'], ascending=[False, True])\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nested dictionary of dicts can be passed to a DataFrame as well. \n",
    "# The indices of the outer dictionary are taken as the the columns\n",
    "# and the inner keys. i.e. the keys of the nested dictionaries,\n",
    "# are used as the row indices:\n",
    "\n",
    "growth = {\"Switzerland\": {\"2010\": 3.0, \"2011\": 1.8, \"2012\": 1.1, \"2013\": 1.9},\n",
    "          \"Germany\": {\"2010\": 4.1, \"2011\": 3.6, \"2012\":\t0.4, \"2013\": 0.1},\n",
    "          \"France\": {\"2010\":2.0,  \"2011\":2.1, \"2012\": 0.3, \"2013\": 0.3},\n",
    "          \"Greece\": {\"2010\":-5.4, \"2011\":-8.9, \"2012\":-6.6, \"2013\":\t-3.3},\n",
    "          \"Italy\": {\"2010\":1.7, \"2011\":\t0.6, \"2012\":-2.3, \"2013\":-1.9}\n",
    "          } \n",
    "growth_frame = pd.DataFrame(growth)\n",
    "growth_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing\n",
    "growth_frame.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying\n",
    "All the rows which are having population greater than 2 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame[city_frame['population'] > 2000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering with multiple conditions using \n",
    "* and - &\n",
    "* or - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame[ (city_frame['population'] > 2000000) & (city_frame['area'] < 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting custom index from a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = city_frame.set_index('cityname')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.loc['Warsaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.loc[['London', 'Hamburg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple columns as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = city_frame.set_index(['cityname', 'country'])\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc[('Warsaw', 'Poland')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc[[('Warsaw', 'Poland'), ('Milan', 'Italy')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.sort_index(ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate, Merge, Join\n",
    "#### Concatenate\n",
    "\n",
    "   The concat function (in the main pandas namespace) does all of the heavy lifting of performing concatenation\n",
    "   \n",
    "   Operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.\n",
    "   \n",
    "   Note that I say “if any” because there is only a single possible axis of concatenation for Series.\n",
    "   \n",
    "```python\n",
    "pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None,    verify_integrity=False, copy=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "index=[4, 5, 6, 7])\n",
    "\n",
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "index=[8, 9, 10, 11])\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2, df3]\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames, axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "'F': ['D4', 'D5', 'D6', 'D7']},\n",
    "index=[2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], sort=False) # default axis=0, join='outer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2],axis=0, join='outer', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2],axis=0, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2],axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2],axis=0, join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MERGE\n",
    "\n",
    "Pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. Users who are familiar with SQL but new to pandas might be interested in a comparison with SQL.\n",
    "\n",
    "Pandas provides a single function, merge, as the entry point for all standard database join operations between DataFrame objects.\n",
    "\n",
    "<b>Syntax</b>:\n",
    "\n",
    "```python\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "df2 = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "'D': ['D0', 'D1', 'D2', 'D3']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer', on=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer', on=['key1', 'key2'])\n",
    "df1 = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2', 'K1'],\n",
    "'key2': ['K0', 'K1', 'K0', 'K1', 'K0'],\n",
    "'A': ['A0', 'A1', 'A2', 'A3', 'A4'],\n",
    "'B': ['B0', 'B1', 'B2', 'B3', 'B4']})\n",
    "\n",
    "df2 = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2', 'K1'],\n",
    "'key2': ['K0', 'K0', 'K0', 'K0', 'K0'],\n",
    "'C': ['C0', 'C1', 'C2', 'C3', 'C4'],\n",
    "'D': ['D0', 'D1', 'D2', 'D3', 'D4']})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer', on=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "df2 = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "'D': ['D0', 'D1', 'D2', 'D3']})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='inner', on=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, how='left', on=['key1', 'key2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, how='right', on=['key1', 'key2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df1, df2, on='key1', suffixes=('_1', '_2'))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df1, df2, on='key1', suffixes=('_df1', '_df2'))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JOIN\n",
    "DataFrame.join is a convenient method for combining the columns \n",
    "of two potentially differently-indexed DataFrames into a single \n",
    "result DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "'B': ['B0', 'B1', 'B2']},\n",
    "index=['K0', 'K1', 'K2'])\n",
    "\n",
    "right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n",
    "'D': ['D0', 'D2', 'D3']},\n",
    "index=['K0', 'K2', 'K3'])\n",
    "\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Alpha':   ['A', 'B', 'A', 'A', 'C', 'B', 'C', 'A', 'C', 'B'],\n",
    "                    'Value1': [1,    5,   7,   3,   2,   5,   9,   1,   5,   3],\n",
    "                    'Value2': [3,    4,   2,   1,   7,   7,   2,   6,   2,   4]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Alpha').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Alpha').min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Alpha').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Alpha', 'Value1']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and Covariance    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance and correlation can also be applied to a single dataframe. In this case, they return their corresponding matrices in the form of two new dataframe objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame([[1,4,3,6],[4,5,6,1],[3,3,1,5],[4,1,6,4]],index=['red','blue','yellow','white'],columns=['ball','pen','pencil','paper'])\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the corrwith() method, you can calculate the pairwise correlations between the columns or rows of a dataframe with a series or another DataFrame()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series([0,1,2,3,9],index=['red','blue','yellow','white','green'])\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.corrwith(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame2 = pd.DataFrame(np.arange(16).reshape((4,4)),index=['red','blue','yellow','white'],columns=['ball','pen','pencil','paper'])\n",
    "frame2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposition of a DataFrame\n",
    "frame2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(frame2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic and Data Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame+frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <b>apply() function</b>, you can apply the function just defined on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x.max() - x.min()\n",
    "\n",
    "frame2.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dataframe, it is possible to define a hierarchical index both for the rows and for the columns. \n",
    "\n",
    "At the time the dataframe is declared, you have to define an array of arrays for the index and columns options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe = pd.DataFrame(np.random.randn(16).reshape(4,4),index=[['white','white','red','red'], ['up','down','up','down']],columns=[['pen','pen','paper','paper'],[1,2,1,2]])\n",
    "mframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe.columns.names = ['objects','id']\n",
    "mframe.index.names = ['colors','status']\n",
    "mframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reordering and Sorting Levels                                              \n",
    "Occasionally, you might need to rearrange the order of the levels on an axis or sort for values at a specific level.                                               The swaplevel() function accepts as arguments the names assigned to the two levels that you want to interchange and returns a new object with the two levels interchanged between them, while leaving the data unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe.swaplevel('colors','status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, the sort_index() function orders the data considering only those of a certain level by specifying it as parameter.\n",
    "\n",
    "As regards the DataFrame, the sorting can be performed independently on each of its two axes. So if you want to order by row following the indexes, just continue to use the function <b>sort_index( )</b> without arguments as you’ve seen before, or if you prefer to order by columns, you will need to use the axis options set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe.sort_index(level='colors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe.sum(level='colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to create a statistic for a given level of the column, for example, the id, you must specify the second axis as an argument through the axis option set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mframe.sum(level='id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Exporting CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(10, 5),\n",
    "                  columns=['a', 'b', 'c', 'd', 'e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('random_data.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('random_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('random_data.csv', skiprows=[2], nrows=3)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Exporting EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df.to_excel('random_data.xlsx', sheet_name='first_sheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('random_data.xlsx', 'first_sheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('random_data.xlsx', 'first_sheet', usecols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_excel('random_data.xlsx', 'first_sheet', usecols=[0, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('random_data.xlsx', 'first_sheet', converters={'b': str})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cfun = lambda x: x if x > 0 else 0\n",
    "\n",
    "pd.read_excel('random_data.xlsx', 'first_sheet', converters={'b': cfun})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Exporting JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) has become one of the most common standard formats, especially for the transmission of data through the Web. So it is normal to have to do with this data format if you want to use the available data on the Web.\n",
    "\n",
    "The special feature of this format is its great flexibility, though its structure is far from being the one to which you are well accustomed, i.e., tabular.\n",
    "\n",
    "When we have a DataFrame and we need to convert it into a JSON file. So, define a DataFrame and then call the to_json() function on it, passing as argument the name of the file that you want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.arange(16).reshape(4,4), index=[\"white\",\"black\",\"red\",\"blue\"],columns=[\"up\",\"down\",\"right\",\"left\"])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_json(\"frame.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(\"frame.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example you have seen is a fairly simple case in which the JSON data were in tabular form (since the file frame.json comes from a DataFrame). Generally, however, the JSON files do not have a tabular structure. Thus, you will need to somehow convert the structure dict file in tabular form. You can refer this process as normalization.\n",
    "\n",
    "The library pandas provides a function, called json_normalize(), that is able to convert a dict or a list in a table. First you have to import the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('bookslist.json','r')\n",
    "text = file.read()\n",
    "text = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_normalize(text,'books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_normalize(text,'books',['writer','nationality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Writing data to sql database(MySQL):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(10, 5),\n",
    "                  columns=['a', 'b', 'c', 'd', 'e'])\n",
    "print (df)\n",
    "\n",
    "engine = sqlalchemy.create_engine('mysql+mysqlconnector://naren:Python@7@localhost/test')\n",
    "con = engine.connect()\n",
    "df.to_sql('random_table_1', con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reading data from sql database(MySQL):__\n",
    "\n",
    "_Reading SQL table:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "engine = sqlalchemy.create_engine('mysql+mysqlconnector://naren:Python@7@localhost/test')\n",
    "con = engine.connect()\n",
    "df = pd.read_sql_table('random_table_1', con)\n",
    "con.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Running SQL query:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "engine = sqlalchemy.create_engine('mysql+mysqlconnector://naren:Python@7@localhost/test')\n",
    "con = engine.connect()\n",
    "df = pd.read_sql_query('select a, c, e from random_table_1 limit 5', con)\n",
    "con.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "engine = sqlalchemy.create_engine('mysql+mysqlconnector://naren:Python@7@localhost/test')\n",
    "con = engine.connect()\n",
    "df = pd.read_sql('select a, c, e from random_table_1 limit 5', con)\n",
    "con.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Writing data to mongo database:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"localhost\",27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.mydatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client['mydatabase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.mycollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['mycollection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is the time to load the data in the collection. Create a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame( np.arange(20).reshape(4,5),columns=['white','red','blue','black','green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before being added to a collection, it must be converted into a JSON format. The conversion process is not as direct as you might imagine; this is because you need to set the data to be recorded on DB and at the same time in order to be re-extract as DataFrame as fairly and as simply as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = json.loads(frame.T.to_json()).values()\n",
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are finally ready to insert a document in the collection, and you can do this with the insert() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.mydocument.insert_many(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you have an object for each line recorded. Now that the data has been loaded into the document within the MongoDB database, you can execute the reverse process, i.e., reading data within a document and then converting them to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = collection['mydocument'].find()\n",
    "dataframe = (list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(dataframe)\n",
    "del dataframe['_id']\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('random_data.csv')\n",
    "df.loc[3:7, 'c'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
